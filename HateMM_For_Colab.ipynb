{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-70ogSzwoISFlUvr6kXBoyFl6LSNhFgk",
      "authorship_tag": "ABX9TyM3TdJe5QRKb77lXo3Xcgqi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDonohoe101/MSc-Research-Project-Scripts/blob/main/HateMM_For_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Imports --\n",
        "import whisper\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, logging\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip"
      ],
      "metadata": {
        "id": "6XEvoi2YHMbg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-- Global constants and variables --\n",
        "VIDEO_FOLDER_PATH = \"/content/drive/MyDrive/MSC RESEARCH PROJECT for COLAB/Datasets/HateMM - Data/7799469/func_test\"\n",
        "ANNOTATION_FILE_PATH = \"/content/drive/MyDrive/MSC RESEARCH PROJECT for COLAB/Datasets/HateMM - Data/7799469/HateMM_annotation.csv\""
      ],
      "metadata": {
        "id": "ZVgkBra1IAKl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Image caption model initialisation --\n",
        "logging.set_verbosity_error()  # silence info & warnings\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device); # ';' suppresses printing model details"
      ],
      "metadata": {
        "id": "YzyWmRu0INvI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility Functions**\n",
        "*These functions help process the videos in to text across various modalities.*\n"
      ],
      "metadata": {
        "id": "EtjMsGC3MZP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_video_to_audio(video_name: str) -> AudioFileClip:\n",
        "    \"\"\"\n",
        "    Converts any valid video format in to mp3 audio to be\n",
        "    fed in to OpenAIs Whisper model later.\n",
        "\n",
        "    Params: video_name (str)\n",
        "    Name of the video file.\n",
        "\n",
        "    Returns: Audio file of type AudioFileClip.\n",
        "    \"\"\"\n",
        "    video = get_video(video_name)\n",
        "    audio = video.audio\n",
        "    #TODO folderise each video such that they will each contain all of its processed info\n",
        "\n",
        "    return audio\n",
        "\n",
        "def get_video(video_name: str) -> VideoFileClip:\n",
        "    \"\"\"\n",
        "    Retrieves path to the video and converts it to a\n",
        "    video file to be processed programatically.\n",
        "\n",
        "    Params: video_name (str)\n",
        "    Name of the video file.\n",
        "\n",
        "    Returns: Video file of type VideoFileClip.\n",
        "    \"\"\"\n",
        "    path_to_video = os.path.join(VIDEO_FOLDER_PATH, video_name)\n",
        "    video = VideoFileClip(path_to_video)\n",
        "    return video\n",
        "\n",
        "def get_all_annotations() -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves the csv file of all annotations and converts\n",
        "    them in to a list.\n",
        "\n",
        "    Returns: Dictionary of all annotations.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(ANNOTATION_FILE_PATH)\n",
        "    annotations_dict = data.to_dict(orient='records')\n",
        "    return annotations_dict\n",
        "\n",
        "def get_annotation(video_name: str, annotations: dict) -> str|None:\n",
        "    \"\"\"\n",
        "    Queries the annotation of the specified video and\n",
        "    returns it as string. May consider returning ALL\n",
        "    labels instead of just the hate/non-hate label.\n",
        "\n",
        "    Params:\n",
        "    video_name (str) - name of the video we are querying.\n",
        "    annotations (dict) - all annotations of the dataset.\n",
        "\n",
        "    Returns: Annotation as string or None if record doesn't exist.\n",
        "    \"\"\"\n",
        "    record = next((row for row in annotations if row['video_file_name'] == video_name), None)\n",
        "    if record:\n",
        "        return str(record.get('label'))\n",
        "    return None\n",
        "\n",
        "def get_video_frames(video_name: str):\n",
        "    \"\"\"\n",
        "    Creates a directory for the extracted frames, samples the frames\n",
        "    from the video provided, saves them in to the folder created.\n",
        "\n",
        "    Params: video_name\n",
        "    Name of the video we are processing.\n",
        "    \"\"\"\n",
        "    # Load the video\n",
        "    path_to_video = os.path.join(VIDEO_FOLDER_PATH, video_name)\n",
        "    cap = cv2.VideoCapture(path_to_video)\n",
        "    base_name = os.path.splitext(video_name)[0]  # removes extension\n",
        "    frame_folder = os.path.join(VIDEO_FOLDER_PATH, f\"{base_name}_frames\")\n",
        "    os.makedirs(frame_folder, exist_ok=True)\n",
        "\n",
        "    # Check if video was opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        exit()\n",
        "\n",
        "    frame_count = 0  # Initialize frame counter\n",
        "    SAMPLE_RATE = 24 # how many frames before sampling next\n",
        "    # Loop through each frame in the video\n",
        "    while True:\n",
        "        success, frame = cap.read()\n",
        "\n",
        "        # Break the loop if the video ends\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        if frame_count % SAMPLE_RATE == 0:\n",
        "            # Save the frame as an image\n",
        "            frame_filename = os.path.join(frame_folder,f\"{base_name}_frame_{frame_count:04d}.jpg\")\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            #print(f\"Frame {frame_count} saved as {frame_filename}\")#debug\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "def get_audio_transcript(video_name: str) -> str|list:\n",
        "    \"\"\"\n",
        "    Writes an audio file to the system and then uses\n",
        "    the Whisper model to transcribe the audio in to\n",
        "    textual format.\n",
        "\n",
        "    Params: video_name\n",
        "    Name of the video file.\n",
        "\n",
        "    Returns: Transcribed audio in textual format of type string OR list.\n",
        "    \"\"\"\n",
        "    #Retrieves audio from video, then writes audio file to users system\n",
        "    audio = convert_video_to_audio(video_name)\n",
        "    audio_path = os.path.join(VIDEO_FOLDER_PATH, os.path.splitext(video_name)[0])+\"_audio.mp3\"\n",
        "    audio.write_audiofile(audio_path, logger=None)\n",
        "\n",
        "    #Load model and extract audio transcription given the audio files' path\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    audio_transcript = result[\"text\"]\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return audio_transcript\n",
        "\n",
        "#TODO put this in the other py file\n",
        "\n",
        "def generate_caption(processor: BlipProcessor, model: BlipForConditionalGeneration, image_path: str):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(**inputs)\n",
        "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "def output_captions(video_name: str):\n",
        "    output_captions = {}\n",
        "    frame_folder = os.path.join(VIDEO_FOLDER_PATH, f\"{os.path.splitext(video_name)[0]}_frames\")\n",
        "\n",
        "    for filename in os.scandir(frame_folder):\n",
        "        frame_path = os.path.join(frame_folder, filename)\n",
        "        caption = generate_caption(processor, model, frame_path)\n",
        "        print(f\"{filename.name} CAPTION: {caption}\")\n",
        "        output_captions[filename] = caption\n",
        "    return output_captions\n",
        "\n",
        "#TODO\n",
        "def export_to_folder(data):\n",
        "    return 0"
      ],
      "metadata": {
        "id": "tLU4GZUv0L3n"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Loop**\n",
        "This code acts as a high level overview of the data pipeline."
      ],
      "metadata": {
        "id": "KsZY9DZfNAqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "v0_A7XUSxxeF"
      },
      "outputs": [],
      "source": [
        "def process_videos() -> None:\n",
        "    \"\"\"\n",
        "    This is the main function which extracts multimodal semantic features from\n",
        "    videos in to prompts that can be fed in to both open and closed\n",
        "    source LLMs.\n",
        "\n",
        "    Params:\n",
        "    VIDEO_FOLDER_PATH - Path to the video folder.\n",
        "    ANNOTATION_FILE_PATH - Path to the annotation csv file.\n",
        "    \"\"\"\n",
        "\n",
        "    total_extracted_video_data_list = [] #list to hold all videos extracted data\n",
        "    VALID_EXTENSIONS = ['.mp4', '.mov', '.avi', '.mkv'] #checking formats avoids trying to process hidden files\n",
        "\n",
        "    all_annotations_dict = get_all_annotations()\n",
        "\n",
        "    # -- Main loop --\n",
        "    for video in os.scandir(VIDEO_FOLDER_PATH):\n",
        "        if video.is_file() and os.path.splitext(video.name)[1].lower() in VALID_EXTENSIONS:  # check if it's a (valid) video file\n",
        "            video_name = video.name\n",
        "\n",
        "            # -- Calls to the extraction logic in util file --\n",
        "            video_frames = get_video_frames(video_name)\n",
        "            video_audio_transcript = get_audio_transcript(video_name)\n",
        "            print(f\"\\nVIDEO: \", video_name)\n",
        "            print(f\"AUDIO TRANSCRIPT: \",video_audio_transcript,'\\n')#debug statement, remove later\n",
        "            video_visual_description = output_captions(video_name)\n",
        "            video_annotation = get_annotation(video_name, all_annotations_dict)\n",
        "            print(f\"ANNOTATION: \", video_annotation,'\\n')\n",
        "\n",
        "            extracted_video_data = { #Dictionary used to hold all extracted data in a video\n",
        "                \"video_name\" : video_name,\n",
        "                \"video_frames\" : video_frames,\n",
        "                \"video_audio_transcript\" : video_audio_transcript,\n",
        "                \"video_visual_description\" : video_visual_description,\n",
        "                \"video_annotation\" : video_annotation\n",
        "            }\n",
        "\n",
        "            total_extracted_video_data_list.append(extracted_video_data)\n",
        "            torch.cuda.empty_cache()\n",
        "            #time.sleep(0.01)  #debug statement for progress bar\n",
        "\n",
        "    export_to_folder(total_extracted_video_data_list) #call to export logic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Initial logic --\n",
        "if __name__ == \"__main__\":\n",
        "    #Defines the path to the video folder and update the path in the utils module.\n",
        "    process_videos()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwZWkhCdHwwc",
        "outputId": "7ceb7cb9-f9a7-49dc-9e06-614426c7cbc3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "VIDEO:  hate_video_36.mp4\n",
            "AUDIO TRANSCRIPT:   what did you say Isaac? Gorilla. What is it? It's a gorilla. That's a gorilla. Is what a gorilla? That something's new inside. It's a gorilla. What is it? It's a gorilla. \n",
            "\n",
            "hate_video_36_frame_0024.jpg CAPTION: a man is sitting at a desk with a computer\n",
            "hate_video_36_frame_0000.jpg CAPTION: a man is sitting at a desk with a laptop\n",
            "hate_video_36_frame_0144.jpg CAPTION: a little boy is playing with a toy\n",
            "hate_video_36_frame_0168.jpg CAPTION: a little boy is playing with a toy\n",
            "hate_video_36_frame_0120.jpg CAPTION: a little boy is sitting at a table\n",
            "hate_video_36_frame_0192.jpg CAPTION: a little boy is sitting on the floor\n",
            "hate_video_36_frame_0096.jpg CAPTION: a little boy is playing with a toy\n",
            "hate_video_36_frame_0216.jpg CAPTION: a young boy is sitting on the floor\n",
            "hate_video_36_frame_0072.jpg CAPTION: a little boy is playing with a toy\n",
            "hate_video_36_frame_0048.jpg CAPTION: a little boy is sitting on a chair\n",
            "hate_video_36_frame_0264.jpg CAPTION: a young boy is sitting at a table\n",
            "hate_video_36_frame_0312.jpg CAPTION: a young boy is sitting at a table\n",
            "hate_video_36_frame_0336.jpg CAPTION: a little boy is sitting on the floor\n",
            "hate_video_36_frame_0240.jpg CAPTION: a little boy is sitting on the floor\n",
            "hate_video_36_frame_0288.jpg CAPTION: a young boy is sitting at a table\n",
            "hate_video_36_frame_0384.jpg CAPTION: a young boy is sitting at a table\n",
            "hate_video_36_frame_0360.jpg CAPTION: a young boy is sitting at a table\n",
            "hate_video_36_frame_0456.jpg CAPTION: a baby is sitting on a table with a man\n",
            "hate_video_36_frame_0432.jpg CAPTION: a young boy is sitting at a table\n",
            "hate_video_36_frame_0408.jpg CAPTION: a young boy is sitting at a table\n",
            "hate_video_36_frame_0480.jpg CAPTION: a woman is pointing at a computer screen\n",
            "hate_video_36_frame_0504.jpg CAPTION: a cat is sitting on the floor in front of a computer\n",
            "hate_video_36_frame_0576.jpg CAPTION: a cat is sitting on a shelf next to a book\n",
            "hate_video_36_frame_0552.jpg CAPTION: a man is seen in a video posted by the police\n",
            "hate_video_36_frame_0528.jpg CAPTION: a man is sitting at a desk with a computer\n",
            "hate_video_36_frame_0648.jpg CAPTION: a dog is sitting on a table next to a sign\n",
            "hate_video_36_frame_0624.jpg CAPTION: a sign that says, ' making difference '\n",
            "hate_video_36_frame_0600.jpg CAPTION: a sign that says, ' i ' m ' is on the shelf\n",
            "hate_video_36_frame_0672.jpg CAPTION: a sign that says, ' stop the killing of the gorilla '\n",
            "hate_video_36_frame_0696.jpg CAPTION: a dog is sitting on a table next to a poster\n",
            "hate_video_36_frame_0720.jpg CAPTION: a box of medicine sitting on a shelf\n",
            "ANNOTATION:  Hate \n",
            "\n"
          ]
        }
      ]
    }
  ]
}